{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546fb717-dcbb-4ccc-bd1a-d2677be9599e",
   "metadata": {},
   "source": [
    "## Sentence Transformers & Multi-Task Learning - ML Apprentice Project\n",
    "### The goal of this exercise is to assess your ability to implement, train, and optimize neural network architectures, particularly focusing on transformers and multi-task learning extensions. Please explain any and all choices made in the course of this assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb19bf-b72b-443a-9636-82be54ba407d",
   "metadata": {},
   "source": [
    "## Task 1 - Sentence Transformer Implementation\n",
    "#### Implement a sentence transformer model using any deep learning framework of your choice. This model should be able to encode input sentences into fixed-length embeddings. Test your implementation with a few sample sentences and showcase the obtained embeddings. Describe any choices you had to make regarding the model architecture outside of the transformer backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dae43-f528-4bef-bcc4-1afac1e29629",
   "metadata": {},
   "source": [
    "I am using PyTorch due to it's flexibility, its' ease of use in experimental projects like this one, library integration from Hugging Face's Transformers, and debugging ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5798821b-0643-4f3b-aaf2-f22ee068ed3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./venv/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./venv/lib/python3.11/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.11/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./venv/lib/python3.11/site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.11/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# making sure PyTorch and Hugging Face's transformers libraries are installed\n",
    "!pip install torch transformers\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0b5093-f04e-4a14-b0e8-65b26585675f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing relevant libraries for Task 1\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb08331-8a78-484e-acaa-62e922104be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leethornquist/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# I chose BERT-base as my transformer backbone since it's well-established for NLP and effective for building off of as the model gets more advanced\n",
    "# I considered starting with Sentence-BERT but didn't want to pigeon-hole myself from the start\n",
    "\n",
    "transformer_model = \"bert-base-uncased\"\n",
    "\n",
    "# loading the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n",
    "model = AutoModel.from_pretrained(transformer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5598e2-0335-4fe7-a4d9-ac56dfabbdde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a function that takes a batch of sentences as the input, tokenizes it, runs it through my model,\n",
    "# and then puts the token embeddings into fixed-length embeddings\n",
    "\n",
    "def get_sentence_embeddings(sentences):\n",
    "    # batch tokenize the list of sentences\n",
    "    inputs = tokenizer(sentences,\n",
    "                      return_tensors=\"pt\", # need to ensure the tokenizer returns the tokenized data as PyTorch tensors\n",
    "                      truncation=True, # sentence will be truncated if larger than the specified max_length (128) to avoid errors or large computational load\n",
    "                      padding = True, # ensuring all sentences are padded to the same length\n",
    "                       max_length=128) # I chose 128 as my max length since I am controlling the length of the test sentences and they will be relatively small \n",
    "    \n",
    "    # forward pass through the model for the batch to obtain hidden states\n",
    "    outputs = model(**inputs) # \"**\"\" unpacks the dictionary of inputs above into keyword arguments for the forward\n",
    "                            # the output we care most about is the last_hidden_state since it holds the hidden represenations for each token in the input sequence\n",
    "    \n",
    "    # applying mean pooling along the token dimension to get fixed-length sentence embeddings \n",
    "    mean_pool_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return mean_pool_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6025a-c6e8-4da8-96a0-821ff92ac0c9",
   "metadata": {},
   "source": [
    "I chose mean pooling as my embedding method since it's a reliable and straightforward choice for an initial baseline. It averages the token embeddings to produce a balanced representation of the sentence, reducing the influence of any single token (i.e. outliers) that might otherwise dominate the embedding, as can happen with max pooling.\n",
    "\n",
    "I initially had a function that could only process one sentence at a time, but went back to rewrite it to handle batch processing to improve efficiency, to make the model more scaleable, and more consistent as I move to the next step of project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2914490-7033-486a-ac39-00da05532c54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape for sentence 1: torch.Size([768])\n",
      "Embedding shape for sentence 2: torch.Size([768])\n",
      "Embedding shape for sentence 3: torch.Size([768])\n",
      "Embedding shape for sentence 4: torch.Size([768])\n",
      "Embedding shape for sentence 5: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# time to test the function with sample sentences!\n",
    "\n",
    "# list of test sentences\n",
    "test_sentences = [\n",
    "    \"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "    \"Prometheus stole fire from the gods and gave it to man. For this, he was chained to a rock and tortured for eternity.\",\n",
    "    \"Life moves pretty fast. If you don't stop and look around once in a while, you could miss it.\",\n",
    "    \"The mystery of life isn't a problem to solve, but a reality to experience.\",\n",
    "    \"On Monday, California firefighters finally contained the spread of the recent wildfires.\"\n",
    "    # having the sentences in a list makes it easy to add additional sentences in the future\n",
    "]\n",
    "\n",
    "# obtaining embeddings for all sentences at once\n",
    "embeddings = get_sentence_embeddings(test_sentences)\n",
    "\n",
    "# printing the shape for each embedding in the batch - assumes shape is [num_sentences, hidden_size]\n",
    "for idx, emb in enumerate(embeddings):\n",
    "    print(f\"Embedding shape for sentence {idx+1}: {emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad6c4a-4c29-45ef-9146-93b9fe336f2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "These results show that the sentence transformer is functioning correctly. Each sentence's embedding is a 768-dimensional vector, which is accurate for a BERT-base model. The sentences are in a fixed-length vector format and are ready for use in further tasks, like multi-task learning in Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692bfc1b-3595-4a04-b669-0d2d76bfde78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch embedding shape: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "# printing the overall shape of the batch to ensure I accounted for all data\n",
    "print(\"Batch embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b8fe9-b3cc-483c-b403-e49b3bda14aa",
   "metadata": {},
   "source": [
    "Result shows we have 5 sentences in our batch each represented by a 768-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc7637-986d-406d-a1ae-49f09bdbc2f8",
   "metadata": {},
   "source": [
    "## Task 2 - Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23a0e6-5755-4aaf-8409-c4f877a9b077",
   "metadata": {},
   "source": [
    "### Goal: Expand the sentence transormer to handle a multi-task learning settting.\n",
    "#### Task A: Sentence Classification - Classify sentences into predefined classes (you can make these up).\n",
    "#### Task B: [Choose another relevant NLP task such as Named Entity Recognition, Sentiment Analysis, etc.] (you can make the labels up)\n",
    "#### Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66eff55d-6ec1-42e4-a74a-152f8fe2adcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import relevant libraries. I already imported 'torch' in Step 1 so just need its' neurel network module\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e055ec-4af9-4f37-81cf-1a835fe2f0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the multi-task model architecture and linear layers\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, transformer_model, hidden_size, num_classes_taskA, num_classes_taskB):\n",
    "        super(MultiTaskModel, self).__init__() # initiliazing nn.Module\n",
    "        self.transformer = transformer_model # storing the shared transformer backbone (BERT)\n",
    "        self.pooling = lambda x: x.mean(dim=1) # mean pooling over the token dimension for sentence embedding\n",
    "        \n",
    "        # Task A - sentence classification head\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes_taskA)\n",
    "        \n",
    "        # Task B - I'm choosing Sentiment Analysis \n",
    "        self.sentiment = nn.Linear(hidden_size, num_classes_taskB)\n",
    "        \n",
    "    \n",
    "    # defining the forward pass function\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.transformer(**inputs) # forward pass through the transformer\n",
    "        pooled_output = self.pooling(outputs.last_hidden_state) # pooling the outputs to get a single fixed-length vector per sentence\n",
    "        \n",
    "        # generating outputs for both Tasks\n",
    "        output_classification = self.classifier(pooled_output)\n",
    "        output_sentiment = self.sentiment(pooled_output)\n",
    "        \n",
    "        return output_classification, output_sentiment\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b306a862-9e3c-4acc-b368-d419382a2afc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A (Classification) output shape: torch.Size([5, 6])\n",
      "Task B (Sentiment Analysis) output shape: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# first defining the model parameters then testing the multi-task model with examples after \n",
    "hidden_size = 768 # BERT-base hidden size\n",
    "num_classes_taskA = 6 # my made up sentence categories \n",
    "num_classes_taskB = 3 # the sentiment analysis labels\n",
    "\n",
    "# instantiating the model\n",
    "multi_task_model = MultiTaskModel(model, hidden_size, num_classes_taskA, num_classes_taskB) # 'model' is the pre-trained transformer from Step 1\n",
    "\n",
    "# tokenizing the test sentences from Task 1 for the multi-task model\n",
    "inputs = tokenizer(test_sentences, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# passing the tokenized inputs to my multi-task model and printing the shape results\n",
    "output_A, output_B = multi_task_model(inputs)\n",
    "print(\"Task A (Classification) output shape:\", output_A.shape)\n",
    "print(\"Task B (Sentiment Analysis) output shape:\", output_B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ec27c-d220-48be-b42f-45eb1e114641",
   "metadata": {},
   "source": [
    "Results are accurate meaning the multi-level model is functioning as expected.\n",
    "- Task A processed a batch of 5 sentences, producing a 6-dimensional output related to my six made up categories.\n",
    "- Task B processed the same batch of 5 sentences, producing a 3-dimensional output related to my three sentiment labels.\n",
    "- The dimensions show the shared transformer backbone and the two task-specific heads are working together properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d54fa8-0ef5-4c44-80e3-3e4fd1bb1def",
   "metadata": {},
   "source": [
    "## Task 3 - Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d6322-8cfc-4ba3-a8ea-97e841874fd3",
   "metadata": {},
   "source": [
    "A) Implications & Advantages of Different Scenarios:\n",
    "1) If the entire network should be frozen\n",
    "    - This means all layers of the model are frozen and would not be changed in any way during training\n",
    "    - Advantages:\n",
    "        - Training is extremely fast because no updates are needed \n",
    "        - Increased stability because I am relying solely on the pre-trained BERT model's representations which is useful for small datasets\n",
    "        \n",
    "2) If only the transformer backbone should be frozen\n",
    "    - This means the transformer backbone is fixed and its weights would not update, while only the task-specific heads are trained\n",
    "    - Advantages:\n",
    "        - Leverages proven and advanced pre-trained features\n",
    "        - More efficient since the deep layers of the transformer are not touched and it focuses on the smaller heads\n",
    "        - Reduces the risk of overfitting on small datasets\n",
    "\n",
    "3) If only one of the task-specific heads should be frozen\n",
    "    - This means only one task-specific head (Task A or Taks B) is static while both the transformer backbone and the other task head are updated\n",
    "    - Advantages:\n",
    "        - I can concentrate updates on the more critical head if the other task already performs well or has larger amounts of data\n",
    "        - If updating one head is causing interference in the model or a reduced performance, freezing this head could help combat these issues\n",
    "\n",
    "Conclusion\n",
    "- Based on this model's needs and parameters, I would freeze the entire network during training. We have a very limited dataset and pretty standard task heads that the BERT-base backbone is well-equipped to handle. Keeping things simple seems like the best option for this project's use case. I would only start focusing on freezing the backbone or task-specific heads if we had a larger dataset and wanted to see if either of those routes would improve accuracy or loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db95088-68c6-4368-89ad-878a1a31370f",
   "metadata": {},
   "source": [
    "B) Approaching the Transfer Learning Process in Different Scenarios:\n",
    "- Transfer learning is useful when we are working with limited labeled data or when our target tasks are similar to those that a pre-trained model was exposed to.\n",
    "1) The choice of a pre-trained model\n",
    "    - When deciding on the pre-trained model, I need to consider how a certain model relates to the dataset I am working with (the type of data and the size of the set) and the use case of the model. In the case of this project, I am working with a very limited set of data that I created myself, and the data's purpose is related to natural language processing, so I want to choose a model that is relevant and known to capture langugage features. This is why I chose the BERT-base model as it's a widely used model in NLP since it's been trained on massive amounts of text, langugage features and data, providing a very strong starting point for my model.\n",
    "\n",
    "2) The layers to freeze/unfreeze\n",
    "    - When deciding on the layers to freeze/unfreeze, I need to decide which layers are most relevant to my task, how efficient I want/need our model to be, the size of my dataset, the challenge-level of a specific task relative to other tasks, overfitting, and the tradeoff between flexibility for task-specific adaptation versus maintaining the pre-trained model's knowledge.\n",
    "    - Freezing the lower levels of the BERT-based model helps capture more general features like common language structures without risking overfitting on small datasets (which is the case for this project). I could then fine-tune the upper layers / task-specific heads since they are more flexibile and specific, and could improve the performance for the classification and sentiment analysis.\n",
    "    - Another option I would explore is implementing layer-wise learning rate decay. I could assign a lower learning rate to the lower or frozen layers and a higher learning rate to the upper layers and task-specific heads. This would preserve the general language knowledge while allowing the task-specific layers to learn more aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003729c-41e7-4a32-b138-6d2e6a29af71",
   "metadata": {},
   "source": [
    "## Task 4 - Training Loop Implementation (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ffd2a5-5b9b-491c-af16-442ea87b3fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing all relevant libraries (even though I already imported some above - just to reiterate what is needed)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0071897e-5186-49ec-a429-8d434e058828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a hypothetical dataset class\n",
    "\n",
    "class HypotheticalDataset(Dataset):\n",
    "    def __init__ (self, sentences, labels_taskA, labels_taskB):\n",
    "        self.sentences = sentences\n",
    "        self.labels_taskA = labels_taskA\n",
    "        self.labels_taskB = labels_taskB\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        labelA = self.labels_taskA[idx]\n",
    "        labelB = self.labels_taskB[idx]\n",
    "        \n",
    "        # tokenizing the sentence using the same tokenizer as task 1\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        \n",
    "        # removing the extra batch dimension added by the tokenizer so each input has the shape [1, seq_length]\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        \n",
    "        return inputs, torch.tensor(labelA), torch.tensor(labelB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e20fdd0d-48eb-4432-9759-9d99b23566ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copying the list of test sentences from task 1 below for easy reference as hypothetical data\n",
    "test_sentences = [\n",
    "    \"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "    \"Prometheus stole fire from the gods and gave it to man. For this, he was chained to a rock and tortured for eternity.\",\n",
    "    \"Life moves pretty fast. If you don't stop and look around once in a while, you could miss it.\",\n",
    "    \"The mystery of life isn't a problem to solve, but a reality to experience.\",\n",
    "    \"On Monday, California firefighters finally contained the spread of the recent wildfires.\"\n",
    "]\n",
    "\n",
    "# mapping my made up sentence categories and sentiment analysis labels\n",
    "taskA_labels = {0: \"Expression\", 1: \"Opinion\", 2: \"Quote\", 3: \"Riddle\", 4: \"News\", 5: \"Other\"}\n",
    "\n",
    "taskB_labels = {0: \"Neutral\", 1: \"Positive\", 2: \"Negative\"}\n",
    "\n",
    "# defining dummy labels \n",
    "dummy_labels_taskA = [3, 2, 2, 2, 4]\n",
    "dummy_labels_taskB = [0, 2, 0, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff48870-b879-48e8-8de8-b1c11396e691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the dataset and dataloader\n",
    "\n",
    "dataset = HypotheticalDataset(test_sentences, dummy_labels_taskA, dummy_labels_taskB)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # 2 samples are processed together in each training iteration. Would want to increase the batch_size for full-scale training to get more stable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb541f3-cb73-415c-9a76-510452fb8600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the optimizer and loss function\n",
    "\n",
    "optimizer = optim.Adam(multi_task_model.parameters(), lr=2e-5) # ensuring the Adam optimizer will update all parameters in the model - can adjust later if I want to freeze parts of the model\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f362943c-7dda-4917-8f19-4f0ab62ba1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Loss = 2.9544, Task A Accuracy: = 0.0000, Task B Accuracy = 0.2000\n",
      "Epoch 2/5: Loss = 2.4422, Task A Accuracy: = 0.8000, Task B Accuracy = 0.6000\n",
      "Epoch 3/5: Loss = 2.0524, Task A Accuracy: = 0.8000, Task B Accuracy = 0.8000\n",
      "Epoch 4/5: Loss = 1.5894, Task A Accuracy: = 1.0000, Task B Accuracy = 0.8000\n",
      "Epoch 5/5: Loss = 1.1579, Task A Accuracy: = 1.0000, Task B Accuracy = 0.8000\n"
     ]
    }
   ],
   "source": [
    "# training loop example just for demonstration since I am not actually training the model\n",
    "\n",
    "num_epochs = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    multi_task_model.train() # setting the model to training mode\n",
    "    epoch_loss = 0\n",
    "    total_samples = 0\n",
    "    correct_taskA = 0\n",
    "    correct_taskB = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        inputs, labels_taskA, labels_taskB = batch\n",
    " \n",
    "        optimizer.zero_grad() # clear previous gradients\n",
    "        \n",
    "        output_A, output_B = multi_task_model(inputs) # forward pass thru the multi-task model so I get outputs for both task A and task B. Same forwarding method as in previous steps\n",
    "        \n",
    "        # computing the loss for each task using CrossEntropyLoss\n",
    "        loss_A = criterion(output_A, labels_taskA)\n",
    "        loss_B = criterion(output_B, labels_taskB)\n",
    "        loss = loss_A + loss_B # combining the losses, but in a real-world model I might want to weigh them depending on how critical each task is \n",
    "        \n",
    "        loss.backward() # backward pass on the total loss\n",
    "        optimizer.step() # update model parameters\n",
    "        \n",
    "        # updating the metrics\n",
    "        batch_size = labels_taskA.size(0)\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # calculating accuracy for Task A\n",
    "        predictions_A = torch.argmax(output_A, dim=1)\n",
    "        correct_taskA += (predictions_A == labels_taskA).sum().item()\n",
    "        \n",
    "        # calculating accuracy for Task B\n",
    "        predictions_B = torch.argmax(output_B, dim=1)\n",
    "        correct_taskB += (predictions_B == labels_taskB).sum().item()\n",
    "    \n",
    "    # computing average loss and accuracy for each epoch. Basic metrics for this example but I could also use F1-score or precision/recall\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    accuracy_taskA = correct_taskA / total_samples\n",
    "    accuracy_taskB = correct_taskB / total_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, Task A Accuracy: = {accuracy_taskA:.4f}, Task B Accuracy = {accuracy_taskB:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3dfc1-d20e-4aab-9328-3f76a90bdd31",
   "metadata": {
    "tags": []
   },
   "source": [
    "- These results will show how the model classifies the data for both tasks. The scores can change each time I run the model since shuffle=True. Most of the time, the accuracy for both tasks will be 0.8 or 1.0 in each epoch due to the extremely small sample size I am using as it's easy to fit, so in a real-world production model I would want to have a much larger set of data to improve the model for use in a more complex environment.\n",
    "- The loss is gradually decreseasing which is great to see because it means the model is effectively optimizing its' parameters on my small dataset. A larger dataset would likely have more variability with the loss (and accuracy).\n",
    "- If I had a larger dataset for training this model, I would experiment with different batch sizes, learning rates, epochs, freezing/unfreezing layers as discussed in Task 3, and implementing more advanced metrics like F1-score and precision/recall to better understand the effectiveness of the model. But, I would need to keep in mind computational efficiency/demands and overfitting as I experiement with these different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1dc85-6a70-4db4-af17-ceece01b48e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
